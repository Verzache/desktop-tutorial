1. To perform data Pre-processing task and demonstrate Classification algorithm of K nearest neighbour for the given dataset.
2. To Demonstrate Classification algorithm of Decision Tree on the given Dataset.
3. To pre-process the data and demonstrate Clustering  algorithm of K-means Clustering.
4. To implement any one Hierarchical Clustering method for the given dataset.
5. Implement Association Rule Mining algorithm (Apriori).
6. To install Power BI and perform data visualization on the dataset.
7. To perform the Extraction Transformation and Loading (ETL) process to construct the database in Power BI.
8. To use the data in Microsoft Excel and create the Pivot table and Pivot Chart.

1. To perform data Pre-processing task and demonstrate Classification algorithm of K nearest neighbour for the given dataset.
*prac1_a
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#Importing the dataset
dataset = pd.read_csv("Social_Network_Ads.csv")
x = dataset.iloc[:,[2,3]].values
y = dataset.iloc[:,-1].values
dataset.head()
dataset.describe()
dataset.info()
dataset.isnull().sum()
#Splitting the dataset into training and testing
from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.20, random_state = 90)
print("Size of x-training data: ", xtrain.shape)
print("Size of y-training data: ", ytrain.shape)
print("Size of x-test data: ", xtest.shape)
print("Size of y-test data: ", ytest.shape)
#Feature scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
xtrain = sc.fit_transform(xtrain)
xtest = sc.transform(xtest)
#Training the knn model on the training set
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 7, metric = "minkowski", p=2)
classifier.fit(xtrain,ytrain)
#Predict the test set result
ypred = classifier.predict(xtest)
#Making confusion matrix
from sklearn.metrics import confusion_matrix , accuracy_score
cm = confusion_matrix(ytest, ypred)
ac = accuracy_score(ytest, ypred)
print("\nConfusion Matrix: \n",cm)
print("Accuracy of the model: ",ac)
#plotting elbow method graph
neighbors = np.arange(1,9)
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))
for i,k in enumerate(neighbors):
    knn = KNeighborsClassifier(n_neighbors = k)
    knn.fit(xtrain, ytrain)
    train_accuracy[i] = knn.score(xtrain, ytrain)
    test_accuracy[i] = knn.score(xtest, ytest)
plt.plot(neighbors, train_accuracy, label="Train Accuracy")
plt.plot(neighbors, test_accuracy, label="Test Accuracy")
plt.legend()
plt.xlabel("n_neighbors")
plt.ylabel("Accuracy")
plt.show()

*prac1_b
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
dataset = pd.read_csv("BankNote_Authentication.csv")
x=dataset.iloc[:,[0,1,2,3]].values
y=dataset.iloc[:,-1].values
print(dataset.count)
from sklearn.model_selection import train_test_split
xtrain, xtest,ytrain,ytest = train_test_split(x,y,test_size = 0.20,random_state=40)
 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
xtrain = sc.fit_transform(xtrain)
xtest = sc.transform(xtest)
 
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 3, metric = "minkowski", p=2)
classifier.fit(xtrain,ytrain)
 
ypred = classifier.predict(xtest)
 
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(ytest, ypred)
ac = accuracy_score(ytest, ypred)
print("Confusion Matrix: ",cm)
print("Accuracy Score: ", ac)
 
#plotting elbow method graph
neighbors = np.arange(1,20)
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))
 
for i,k in enumerate(neighbors):
    knn = KNeighborsClassifier(n_neighbors = k)
    knn.fit(xtrain, ytrain)
    train_accuracy[i] = knn.score(xtrain, ytrain)
    test_accuracy[i] = knn.score(xtest, ytest)
    
plt.plot(neighbors, train_accuracy, label="Train Accuracy")
plt.plot(neighbors, test_accuracy, label="Test Accuracy")
plt.legend()
plt.xlabel("n_neighbors")
plt.ylabel("Accuracy")
plt.show()
------------------------
2. To Demonstrate Classification algorithm of Decision Tree on the given Dataset.
*prac2_a
import pandas as pd    
#importing datasets  
  
dataset= pd.read_csv('./Social_Network_Ads.csv')  
#Extracting Independent and dependent Variable  
x= dataset.iloc[:, [2,3]].values  
y= dataset.iloc[:, 4].values  
dataset.count
  
# Splitting the dataset into training and test set.  
from sklearn.model_selection import train_test_split  
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25, random_state=0)  
  
#feature Scaling  
from sklearn.preprocessing import StandardScaler    
st_x= StandardScaler()  
x_train= st_x.fit_transform(x_train)    
x_test= st_x.transform(x_test)    

#Fitting Decision Tree classifier to the training set  
from sklearn.tree import DecisionTreeClassifier  
classifier= DecisionTreeClassifier(criterion='entropy', random_state=0)
classifier.fit(x_train, y_train)


#Predicting the test set result  
y_pred= classifier.predict(x_test)  
#Creating the Confusion matrix  
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn import tree
cm = confusion_matrix(y_test, y_pred)
ac = accuracy_score(y_test, y_pred)
print ("Confusion Matrix : \n", cm)
print ("Accuracy : ", ac)
tree.plot_tree(classifier)
-----------------------
3. To pre-process the data and demonstrate Clustering  algorithm of K-means Clustering.
Code: Without Dataset
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
x, y = make_blobs(n_samples=300, centers=6, cluster_std=0.60, random_state=0)
plt.scatter(x[:,0], x[:,1])
wcss=[]
for i in range(1,11):
   kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=100)
   kmeans.fit(x)
 wcss.append(kmeans.inertia_)
plt.plot(range(1,11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=100)
pred_y = kmeans.fit_predict(x)
plt.scatter(x[:,0], x[:,1])
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')
plt.show()

Code: With Database
*prac3_b
import numpy as nm    
import matplotlib.pyplot as mtp    
import pandas as pd   
# Importing the dataset  
dataset = pd.read_csv('Wholesale customers data.csv')  
x = dataset.iloc[:, [3, 4]].values

#finding optimal number of clusters using the elbow method  
from sklearn.cluster import KMeans  
wcss_list= []  #Initializing the list for the values of WCSS  
  
#Using for loop for iterations from 1 to 10.  
for i in range(1, 11):  
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state= 42)  
    kmeans.fit(x)  
    wcss_list.append(kmeans.inertia_)  
mtp.plot(range(1, 11), wcss_list)  
mtp.title('The Elobw Method Graph')  
mtp.xlabel('Number of clusters(k)')  
mtp.ylabel('wcss_list')  
mtp.show()  

#training the K-means model on a dataset  
kmeans = KMeans(n_clusters=5, init='k-means++', random_state= 42)  
y_predict= kmeans.fit_predict(x)  

#visulaizing the clusters  
mtp.scatter(x[y_predict == 0, 0], x[y_predict == 0, 1], s = 100, c = 'blue', label = 'Cluster 1') #for first cluster  
mtp.scatter(x[y_predict == 1, 0], x[y_predict == 1, 1], s = 100, c = 'green', label = 'Cluster 2') #for second cluster  
mtp.scatter(x[y_predict== 2, 0], x[y_predict == 2, 1], s = 100, c = 'red', label = 'Cluster 3') #for third cluster  
mtp.scatter(x[y_predict == 3, 0], x[y_predict == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4') #for fourth cluster  
mtp.scatter(x[y_predict == 4, 0], x[y_predict == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5') #for fifth cluster  
mtp.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroid')   
mtp.title('Clusters of customers')  
mtp.xlabel('Annual Income (k$)')  
mtp.ylabel('Spending Score (1-100)')  
mtp.legend()  
mtp.show()  
-------------------
4. To implement any one Hierarchical Clustering method for the given dataset.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
x = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]
y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]
plt.scatter(x, y)
plt.show()
from scipy.cluster.hierarchy import dendrogram, linkage
data = list(zip(x, y))
print(data)
linkage_data = linkage(data, method='ward', metric='euclidean')
dendrogram(linkage_data)
plt.show()
hierarchical_cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
labels = hierarchical_cluster.fit_predict(data)
plt.scatter(x, y, c=labels,cmap ='rainbow')
plt.show()
data = pd.read_csv('E:/SEM-5/Data Warehousing and Mining/Wholesale customers data(1).csv')
data.head()
from sklearn.preprocessing import normalize
data_scaled = normalize(data)
data_scaled = pd.DataFrame(data_scaled, columns=data.columns)
data_scaled.head()
import scipy.cluster.hierarchy as shc
plt.figure(figsize=(10, 7))  
plt.title("Dendrograms")  
dend = shc.dendrogram(shc.linkage(data_scaled, method='ward'))
from sklearn.cluster import AgglomerativeClustering
cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')  
cluster.fit_predict(data_scaled)
plt.figure(figsize=(10, 7))  
plt.scatter(data_scaled['Milk'], data_scaled['Grocery'], c=cluster.labels_, cmap ='rainbow') 
-------------------
5. Implement Association Rule Mining algorithm (Apriori).
import numpy as np
import pandas as pd
from apyori import apriori

store_data=pd.read_csv('E:/SEM-5/Data Warehousing and Mining/Day 1.csv',header=None)
print(store_data)
records=[]
for i in range(0,21):
    records.append([str(store_data.values[i,j]) for j in range(0,5)])
print(records)
    
a_rule=apriori(records, min_supprt=0.5, min_confidence=0.7, min_lift=1.2, min_length=2)
a_results=list(a_rule)

print(len(a_results))
print(a_results)

for i in a_results:
    print(i)
    print('\n')
-------------------
6. To install Power BI and perform data visualization on the dataset.
Microsoft Store -> Search Power Bi Desktop -> Install
Data Visualisation on datasat.
	1. Import the Universal bank dataset.
	2. Plot a stacked column chart of Income and education w.r.t. age.
	3. Plot the pie chart of education qualification with age. Provide the zip code details using the tool tip.
	4. Plot the line and column stacked chart showing age and experience and education in column chart and income on line chart.
	5. Modify the above column stacked chart for age group greater than 50.
	6. View age, experience, income data in tabular format using table.
-------------------
7. To perform the Extraction Transformation and Loading (ETL) process to construct the database in Power BI.
	1. Load the Emp1.xlsx
		Click Get Data
		>Select Excel workbook
		>Select “EMP1.xlsx”
		>Select “Sheet1”
		>Click on load
		>Go to home and select transform data to view the sheet
	2. Remove the first row
		>Go to remove rows option 
		>Select remove top rows
		>Enter number of rows to be removed
		>Performed function will be displayed in “Applied steps” at the right corner
	3. Promote the first row as header.
		>Go to “Transform” tab and click on “Use first row as header” 
	4. Remove the null column.
		>Select the empty column
		>Select “remove columns” from the tab
	5. Replace the missing values in salary column with 99.
		>Select Salary column
		>Go to replace values in the tab and enter the required values
	6. Replace the empty spaces in Gender column with Female.
		>Select “Gender” column
		>Select replace values in tab and enter the required values
	7. Remove duplicate data in name column.
		>Select the “name” column. Go to “Remove rows” option in tab and select “remove duplicates”
	8. Change the data type of Year column as text.
		>Select the “year” column
		>Go to “Data type” option in tab and select the “text” option
	9. Convert the data in the Name column to Upper case.	
		>Select “name” column
		>Select “format” option from “Transform” tab and click on Uppercase option
	10. Remove all the extra whitespaces in the Name column. 
		>Select “trim” option in “format” available in “transform” tab
	11. Split the Name column into two.
		>Select “name” column. Go into “transform” tab and click on “split column” option after that choose “By Delimiter” option
		>Go to “advanced options” and enter number of columns to split into
	12. Rename the Name1 column in to First name and Name 2 as Surname.
		>Select “name.1” column. Go to “rename” option in “transform” tab and enter the value. Perform the same for “name.2” column
	13. Change the data type of salary as decimal number.
	14. Change the data type of Start date column as date.
		>Select “Start date” column. Go to transform tab option “data type” and select “data” option
	15. Sort the data in the ascending order of Name
		>Select dropdown option beside “name” column and click on “sort ascending” option
	16. Remove the rows with null in name column
		>Select “first name” column and go to dropdown option besides it and uncheck “null” option 
	17. Save the file.
		>Check the “save” option in dropdown
	18. Remove the staff with department as null.
		>Select dropdown besides “Department” column and uncheck the “null” option
-------------------
8. To use the data in Microsoft Excel and create the Pivot table and Pivot Chart.
	1.	Open AmazingMart.xlsx
	2.	Select OrderBreakdown Sheet
	3.	Select a Pivot Table in new worksheet
		>Click on insert 
		>Click on pivot tables
		>Click OK
	4.	Display the total profit incurred.
	5.	Get the total Profit of each category of items.
	6.	Display the total Profit distribution as per every subcategory inside category of items.
	7.	Display the AVERAGE Profit distribution as per every subcategory inside category of items.
		>Go to value Field Setting
		>Select Average
		>Click OK
	8.	Sort the Profit in descending order.
		>Go to more settings
		>Select Average of Profit
		>Click OK
	9.	Sort the Category and subcategory in the descending order.
		>First sort the Category
		>Then sort the Sub-category
	10.	Use ListOfOrder sheet
	11.	Insert 2 dimensional Pivot table such that countries are available along rows, shipping mode is available in columns along with total amount as values.
	12.	Apply the filter for central region.
		>Add Regions to Filter
		>Click on ALL
		>Select Central
	13.	Display the data in the form of percentage of Grand total.
		>Right click on total cell
		>Select Show value as
		>Click % of Grand Total
	14.	Insert a 3D stacked pyramid Pivot chart for the above table
		>Glick on Pivot chart
		>Select Stacked 3-D
		>Click OK
		>Click on the chart and select Format data series
		>Then select pyramid
	15.	Display the data of only Germany.
		>Go to row labels and select only Germany
	16.	Create pivot table in Power Bi.
	